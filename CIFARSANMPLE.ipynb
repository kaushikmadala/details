{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFARSANMPLE.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaushikmadala/details/blob/master/CIFARSANMPLE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpBN1AKffnsD",
        "colab_type": "code",
        "outputId": "ff9e0d05-4c93-4377-ed54-d51a71c4045d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Seed value\n",
        "# Apparently you may use different seed values at each stage\n",
        "seed_value= 0\n",
        "\n",
        "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "\n",
        "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
        "import random\n",
        "random.seed(seed_value)\n",
        "\n",
        "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
        "import numpy as np\n",
        "np.random.seed(seed_value)\n",
        "\n",
        "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
        "import tensorflow as tf\n",
        "tf.set_random_seed(seed_value)\n",
        "\n",
        "from keras.models import load_model\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "# Split the data\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.15, shuffle= True)\n",
        "\n",
        "model = load_model(\"mnist_model.h5\")\n",
        "preds_train = model.predict(x_train, verbose=0)\n",
        "preds_val = model.predict(x_valid, verbose=0)\n",
        "preds = model.predict(x_test, verbose=0)\n",
        "\n",
        "model_copy = model\n",
        "weights = model.get_weights()\n",
        "weights1 = model.get_weights()\n",
        "print(len(weights))\n",
        "#updated_weights = weights[:]\n",
        "#print(\"RESULTS\", len(updated_weights), id(weights), id(updated_weights))\n",
        "#max_val = max(arr.max() for arr in weights)\n",
        "#min_val = min(arr.min() for arr in weights)\n",
        "#print(\"maxval is\",max_val)\n",
        "#print(\"minval is\",min_val)\n",
        "\n",
        "# need to add conditions that allows arrays of different dimensions\n",
        "# Add code to check if it is an element of a list\n",
        "x = 0.0\n",
        "\n",
        "# Change fopr to while an d check it\n",
        "for r1 in range(0, len(weights)):\n",
        "  print(r1)\n",
        "  max_val = max(arr.max() for arr in weights[r1])\n",
        "  min_val = min(arr.min() for arr in weights[r1])\n",
        "  print(\"maxval is\",max_val)\n",
        "  print(\"minval is\",min_val)\n",
        "  if(len(weights[r1]>1)):\n",
        "    #print(\"In r1\")\n",
        "    for r2 in range(0,len(weights[r1])):\n",
        "      #print(type(weights[r1][r2]))\n",
        "      if((type(weights[r1][r2]) is np.ndarray) and len(weights[r1][r2])>1):\n",
        "        #print(\"in r2\")\n",
        "        for r3 in range(0,len(weights[r1][r2])):\n",
        "          weights[r1][r2][r3] = -weights[r1][r2][r3]+1\n",
        "          #print(\"Updated\")\n",
        "      else:\n",
        "        weights[r1][r2] = -weights[r1][r2]+1\n",
        "        # = x\n",
        "        print(weights1[r1][r2], weights[r1][r2])\n",
        "  else:\n",
        "    weights[r1] = -weights[r1]+1\n",
        "   # print(\"Updated\")\n",
        "     \n",
        "model_copy.set_weights(weights)\n",
        "\n",
        "print(\"W!\", len(weights1),len(weights1[1]),(weights1[1][1]))\n",
        "print(\"w\", len(weights),len(weights[1]),(weights[1][1]))\n",
        "\n",
        "\n",
        "#if(weights==weights1):\n",
        "#  print(\"Did not update\")\n",
        "\n",
        "\n",
        "copy_preds_train = model_copy.predict(x_train,verbose=0)\n",
        "print(\"SAMPLES VALUES\")\n",
        "print(copy_preds_train[1])\n",
        "print(copy_preds_train[6])\n",
        "\n",
        "copy_preds_val = model_copy.predict(x_valid, verbose=0)\n",
        "copy_preds = model_copy.predict(x_test, verbose=0)\n",
        "\n",
        "\n",
        "match_with_copy = 0\n",
        "mismatch_with_copy = 0\n",
        "match_with_actual = 0\n",
        "mismatch_with_actual = 0\n",
        "copy_match_with_actual = 0\n",
        "copy_mismatch_with_actual = 0\n",
        "\n",
        "\n",
        "file = open(\"negativeofweightsplus1file_fashionmnisit_regression_test.txt\",\"w\")\n",
        "for ival in range(len(preds)):\n",
        "  for pval in range(0, len(preds[ival])):\n",
        "    file.write(str(preds[ival][pval]))\n",
        "    file.write(\" \")\n",
        "  for pval in range(0,len(copy_preds[ival])):\n",
        "    file.write(str(copy_preds[ival][pval]))\n",
        "    file.write(\" \")\n",
        "  if(np.where(preds[ival] == np.amax(preds[ival]))!=np.where(y_test[ival] == np.amax(y_test[ival]))):\n",
        "    print(\"Writing zero\")\n",
        "    file.write(\"0\")\n",
        "    mismatch_with_actual = mismatch_with_actual+1\n",
        "  else:\n",
        "    file.write(\"1\")\n",
        "    match_with_actual = match_with_actual+1\n",
        "    \n",
        "  file.write(\"\\n\")\n",
        "  \n",
        "file.close()\n",
        "\n",
        "\n",
        "file = open(\"negativeofweightsplus1file_fashionmnisit_regression_input.txt\",\"w\")\n",
        "\n",
        "for ival in range(len(preds_val)):\n",
        "  for pval in range(0, len(preds_val[ival])):\n",
        "    file.write(str(preds_val[ival][pval]))\n",
        "    file.write(\" \")\n",
        "  for pval in range(0,len(copy_preds_val[ival])):\n",
        "    file.write(str(copy_preds_val[ival][pval]))\n",
        "    file.write(\" \")\n",
        "  if(np.where(preds_val[ival] == np.amax(preds_val[ival]))!=np.where(y_valid[ival] == np.amax(y_valid[ival]))):\n",
        "    print(\"Writing zero\")\n",
        "    file.write(\"0\")\n",
        "    mismatch_with_actual = mismatch_with_actual+1\n",
        "  else:\n",
        "    file.write(\"1\")\n",
        "    match_with_actual = match_with_actual+1\n",
        "    \n",
        "  file.write(\"\\n\")\n",
        "  \n",
        "  if(np.where(copy_preds_val[ival] == np.amax(copy_preds_val[ival]))!=np.where(y_valid[ival] == np.amax(y_valid[ival]))):\n",
        "    copy_mismatch_with_actual = copy_mismatch_with_actual + 1\n",
        "  else:\n",
        "    copy_match_with_actual = copy_match_with_actual + 1\n",
        "    \n",
        "  if(np.where(preds_val[ival] == np.amax(preds_val[ival]))!=np.where(copy_preds_val[ival] == np.amax(copy_preds_val[ival]))):\n",
        "    mismatch_with_copy = mismatch_with_copy+1\n",
        "  else:\n",
        "    match_with_copy = match_with_copy+1    \n",
        "    #print(np.where(preds_val[ival] == np.amax(preds_val[ival])), np.where(copy_preds_val[ival] == np.amax(copy_preds_val[ival])), np.where(y_valid[ival] == np.amax(y_valid[ival,\n",
        "    \n",
        "print(\"Results\")\n",
        "print(\"True Mode vs Actual\")\n",
        "print(\"Match = \", match_with_actual)\n",
        "print(\"Mismatch = \", mismatch_with_actual)\n",
        "\n",
        "print(\"Copy Mode vs Actual\")\n",
        "print(\"Match = \", copy_match_with_actual)\n",
        "print(\"Mismatch = \", copy_mismatch_with_actual)\n",
        "\n",
        "print(\"Copy Mode vs True Mode\")\n",
        "print(\"Match = \", match_with_copy)\n",
        "print(\"Mismatch = \", mismatch_with_copy)\n",
        "\n",
        "\n",
        "match_with_copy = 0\n",
        "mismatch_with_copy = 0\n",
        "match_with_actual = 0\n",
        "mismatch_with_actual = 0\n",
        "copy_match_with_actual = 0\n",
        "copy_mismatch_with_actual = 0\n",
        "\n",
        "\n",
        "\n",
        "for ival in range(0,len(preds_train)):\n",
        "  for pval in range(0, len(preds_train[ival])):\n",
        "    file.write(str(preds_train[ival][pval]))\n",
        "    file.write(\" \")\n",
        "  for pval in range(0,len(copy_preds_train[ival])):\n",
        "    file.write(str(copy_preds_train[ival][pval]))\n",
        "    file.write(\" \")\n",
        "  if(np.where(preds_train[ival] == np.amax(preds_train[ival]))!=np.where(y_train[ival] == np.amax(y_train[ival]))):\n",
        "    file.write(\"0\")\n",
        "    mismatch_with_actual = mismatch_with_actual+1\n",
        "  else:\n",
        "    file.write(\"1\")\n",
        "    match_with_actual = match_with_actual+1\n",
        "   \n",
        "  file.write(\"\\n\")\n",
        "  if(np.where(copy_preds_train[ival] == np.amax(copy_preds_train[ival]))!=np.where(y_train[ival] == np.amax(y_train[ival]))):\n",
        "    copy_mismatch_with_actual = copy_mismatch_with_actual + 1\n",
        "  else:\n",
        "    copy_match_with_actual = copy_match_with_actual + 1\n",
        "  if(np.where(preds_train[ival] == np.amax(preds_train[ival]))!=np.where(copy_preds_train[ival] == np.amax(copy_preds_train[ival]))):\n",
        "    mismatch_with_copy = mismatch_with_copy+1\n",
        "  else:\n",
        "    match_with_copy = match_with_copy+1    \n",
        "    #print(np.where(preds_val[ival] == np.amax(preds_val[ival])), np.where(copy_preds_val[ival] == np.amax(copy_preds_val[ival])), np.where(y_valid[ival] == np.amax(y_valid[ival,\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Training Results\")\n",
        "print(\"True Mode vs Actual\")\n",
        "print(\"Match = \", match_with_actual)\n",
        "print(\"Mismatch = \", mismatch_with_actual)\n",
        "\n",
        "print(\"Copy Mode vs Actual\")\n",
        "print(\"Match = \", copy_match_with_actual)\n",
        "print(\"Mismatch = \", copy_mismatch_with_actual)\n",
        "\n",
        "print(\"Copy Mode vs True Mode\")\n",
        "print(\"Match = \", match_with_copy)\n",
        "print(\"Mismatch = \", mismatch_with_copy)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "8\n",
            "0\n",
            "maxval is 0.3770912\n",
            "minval is -0.451004\n",
            "1\n",
            "maxval is 0.1364974\n",
            "minval is -0.034754965\n",
            "0.1364974 0.8635026\n",
            "0.025890594 0.9741094\n",
            "-0.034754965 1.034755\n",
            "-0.006958223 1.0069582\n",
            "0.041041296 0.9589587\n",
            "-0.002422107 1.0024221\n",
            "-0.017479245 1.0174793\n",
            "0.051986013 0.94801396\n",
            "0.06629018 0.9337098\n",
            "0.089413 0.910587\n",
            "-0.033373173 1.0333731\n",
            "-0.011621753 1.0116217\n",
            "0.0 1.0\n",
            "0.07486911 0.9251309\n",
            "-0.005242078 1.0052421\n",
            "0.06555217 0.9344478\n",
            "-0.00065696845 1.000657\n",
            "0.008279745 0.99172026\n",
            "0.025940605 0.9740594\n",
            "0.07765344 0.9223466\n",
            "-0.009526756 1.0095267\n",
            "0.015033784 0.9849662\n",
            "0.00061017065 0.9993898\n",
            "-0.010968002 1.010968\n",
            "-0.015430337 1.0154303\n",
            "0.012489267 0.98751074\n",
            "0.047010086 0.95298994\n",
            "0.04159743 0.9584026\n",
            "-0.013145431 1.0131454\n",
            "-0.033625297 1.0336252\n",
            "0.06726579 0.9327342\n",
            "0.030432867 0.9695671\n",
            "2\n",
            "maxval is 0.25972104\n",
            "minval is -0.29104987\n",
            "3\n",
            "maxval is 0.08349224\n",
            "minval is -0.053740002\n",
            "-0.023485057 1.0234851\n",
            "-0.0024213092 1.0024213\n",
            "-0.04808879 1.0480888\n",
            "0.009309013 0.990691\n",
            "0.011968809 0.9880312\n",
            "-0.02811787 1.0281179\n",
            "-0.0033516502 1.0033517\n",
            "-0.04490722 1.0449072\n",
            "0.00925913 0.9907409\n",
            "-0.0020072255 1.0020072\n",
            "0.015500563 0.98449945\n",
            "-0.0012230037 1.001223\n",
            "-0.0072253337 1.0072253\n",
            "-0.023164855 1.0231649\n",
            "-0.007871416 1.0078714\n",
            "-0.042881157 1.0428811\n",
            "0.06329236 0.9367076\n",
            "0.013649866 0.9863501\n",
            "-0.023483686 1.0234836\n",
            "0.018874869 0.9811251\n",
            "0.035577282 0.9644227\n",
            "-0.016916288 1.0169163\n",
            "0.012756551 0.9872435\n",
            "-0.021633107 1.0216331\n",
            "-0.053740002 1.05374\n",
            "0.018610805 0.98138916\n",
            "-0.04513432 1.0451343\n",
            "-0.0032691332 1.0032691\n",
            "0.08349224 0.9165078\n",
            "-0.019154705 1.0191547\n",
            "0.009399334 0.99060065\n",
            "-0.0010605921 1.0010606\n",
            "-0.0018576535 1.0018576\n",
            "0.027897513 0.97210246\n",
            "-0.028605884 1.0286059\n",
            "-0.021014035 1.021014\n",
            "-0.017043196 1.0170432\n",
            "-0.029097438 1.0290974\n",
            "-0.022926806 1.0229268\n",
            "0.009775586 0.9902244\n",
            "-0.0008531399 1.0008532\n",
            "-0.011154774 1.0111548\n",
            "0.018033115 0.9819669\n",
            "-0.014994777 1.0149947\n",
            "-0.01906477 1.0190648\n",
            "-0.0100290915 1.0100291\n",
            "-0.036789585 1.0367895\n",
            "-0.03932333 1.0393233\n",
            "-0.0076229316 1.007623\n",
            "0.021855485 0.9781445\n",
            "0.025448041 0.974552\n",
            "-0.04288147 1.0428815\n",
            "0.013417752 0.9865822\n",
            "0.067999735 0.9320003\n",
            "0.006583895 0.99341613\n",
            "-0.014810253 1.0148102\n",
            "-0.0069749607 1.0069749\n",
            "-0.03182536 1.0318253\n",
            "-0.026633764 1.0266337\n",
            "-0.0022452688 1.0022453\n",
            "-0.03686112 1.0368612\n",
            "-0.016010478 1.0160105\n",
            "-0.0043883068 1.0043883\n",
            "-0.024012174 1.0240122\n",
            "4\n",
            "maxval is 0.1387204\n",
            "minval is -0.12920329\n",
            "5\n",
            "maxval is 0.031676\n",
            "minval is -0.13135691\n",
            "-0.053688608 1.0536886\n",
            "-0.059628535 1.0596285\n",
            "-0.013792038 1.013792\n",
            "-0.07666531 1.0766653\n",
            "-0.05620189 1.0562019\n",
            "-0.06500093 1.0650009\n",
            "-0.05580473 1.0558047\n",
            "-0.037780758 1.0377808\n",
            "-0.047300454 1.0473005\n",
            "-0.012407951 1.0124079\n",
            "-0.014166135 1.0141661\n",
            "0.0027104223 0.9972896\n",
            "0.012904853 0.9870951\n",
            "-0.01867387 1.0186739\n",
            "-0.0384387 1.0384387\n",
            "-0.04473646 1.0447365\n",
            "-0.03915717 1.0391572\n",
            "0.006328734 0.99367124\n",
            "-0.040581085 1.0405811\n",
            "-0.06664096 1.066641\n",
            "0.003393434 0.9966066\n",
            "-0.021920567 1.0219206\n",
            "-0.08231143 1.0823114\n",
            "-0.0527201 1.0527201\n",
            "0.008256877 0.99174315\n",
            "-0.035173588 1.0351735\n",
            "-0.030900182 1.0309002\n",
            "-0.04085799 1.040858\n",
            "-0.08190946 1.0819094\n",
            "-0.05855926 1.0585593\n",
            "-0.052904382 1.0529044\n",
            "0.031676 0.968324\n",
            "-0.03290945 1.0329094\n",
            "-0.016842086 1.0168421\n",
            "-0.024461806 1.0244617\n",
            "-0.041091975 1.0410919\n",
            "-0.07542656 1.0754266\n",
            "-0.011679458 1.0116794\n",
            "-0.102838926 1.1028389\n",
            "-0.06430699 1.064307\n",
            "-0.07810749 1.0781075\n",
            "-0.04250502 1.042505\n",
            "-0.09192959 1.0919296\n",
            "-0.060295526 1.0602956\n",
            "-0.077992156 1.0779922\n",
            "0.011887213 0.9881128\n",
            "-0.03224447 1.0322444\n",
            "-0.020543264 1.0205432\n",
            "-0.07820333 1.0782033\n",
            "-0.064279206 1.0642792\n",
            "-0.03622821 1.0362282\n",
            "-0.0577665 1.0577666\n",
            "-0.021044636 1.0210446\n",
            "-0.07253138 1.0725313\n",
            "-0.012534222 1.0125343\n",
            "-0.03406516 1.0340651\n",
            "-0.07133979 1.0713398\n",
            "-0.07329651 1.0732965\n",
            "-0.07199314 1.0719931\n",
            "0.025251321 0.9747487\n",
            "-0.13135691 1.131357\n",
            "-0.04135715 1.0413572\n",
            "-0.009790519 1.0097905\n",
            "-0.0006290708 1.0006291\n",
            "-0.0349601 1.0349602\n",
            "-0.0058601843 1.0058602\n",
            "-0.08021317 1.0802132\n",
            "-0.030161275 1.0301613\n",
            "-0.046410587 1.0464106\n",
            "-0.06367673 1.0636767\n",
            "-0.08688227 1.0868822\n",
            "-0.03189723 1.0318972\n",
            "-0.07911012 1.0791101\n",
            "-0.053504776 1.0535048\n",
            "-0.007207826 1.0072079\n",
            "-0.0314011 1.0314012\n",
            "-0.04971952 1.0497196\n",
            "-0.03563457 1.0356345\n",
            "-0.058848854 1.0588489\n",
            "-0.011555525 1.0115556\n",
            "-0.044475153 1.0444752\n",
            "-0.020429486 1.0204295\n",
            "0.0040707504 0.99592924\n",
            "-0.0213667 1.0213667\n",
            "-0.028182711 1.0281827\n",
            "-0.020671448 1.0206715\n",
            "-0.0008595511 1.0008595\n",
            "-0.07232645 1.0723264\n",
            "-0.052216567 1.0522165\n",
            "-0.086420365 1.0864204\n",
            "-0.032195363 1.0321953\n",
            "-0.08724147 1.0872414\n",
            "-0.057036046 1.057036\n",
            "-0.0043361615 1.0043361\n",
            "-0.06352953 1.0635295\n",
            "-0.040036473 1.0400364\n",
            "-0.039013762 1.0390137\n",
            "0.0002447475 0.99975526\n",
            "0.00013048988 0.9998695\n",
            "-0.06430425 1.0643042\n",
            "-0.046337124 1.0463371\n",
            "-0.03769279 1.0376928\n",
            "-0.01972646 1.0197265\n",
            "-0.03992654 1.0399265\n",
            "-0.031317845 1.0313178\n",
            "-0.014670674 1.0146707\n",
            "0.0033472066 0.9966528\n",
            "0.0012346705 0.99876535\n",
            "-0.018807031 1.018807\n",
            "-0.06736969 1.0673697\n",
            "-0.043814223 1.0438142\n",
            "-0.017257694 1.0172577\n",
            "-0.034398723 1.0343987\n",
            "-0.079051375 1.0790514\n",
            "-0.035027586 1.0350276\n",
            "-0.054192085 1.0541921\n",
            "-0.01010585 1.0101058\n",
            "-0.011032477 1.0110325\n",
            "-0.12892199 1.128922\n",
            "-0.049891964 1.049892\n",
            "-0.03572622 1.0357262\n",
            "-0.021932263 1.0219322\n",
            "-0.04313497 1.0431349\n",
            "-0.011645786 1.0116458\n",
            "-0.08093444 1.0809344\n",
            "-0.024568064 1.0245681\n",
            "-0.094001815 1.0940018\n",
            "-0.02078783 1.0207878\n",
            "6\n",
            "maxval is 0.31839663\n",
            "minval is -0.46247837\n",
            "7\n",
            "maxval is 0.07647189\n",
            "minval is -0.11763192\n",
            "-0.03911562 1.0391157\n",
            "0.07647189 0.92352813\n",
            "-0.06872378 1.0687238\n",
            "0.062276132 0.9377239\n",
            "-0.0146160405 1.014616\n",
            "0.0014918662 0.99850816\n",
            "-0.11763192 1.1176319\n",
            "-0.06477402 1.064774\n",
            "-0.0016655322 1.0016655\n",
            "0.04316697 0.956833\n",
            "W! 8 32 0.025890594\n",
            "w 8 32 0.9741094\n",
            "SAMPLES VALUES\n",
            "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-f1177939ab7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    177\u001b[0m   \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m   \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy_preds_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mival\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy_preds_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mival\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mival\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mival\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m     \u001b[0mcopy_mismatch_with_actual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy_mismatch_with_actual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsxBB2cNMVhJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q -U tensorflow>=1.8.0\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "# Load the fashion-mnist pre-shuffled train data and test data\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "print(\"x_train shape:\", x_train.shape, \"y_train shape:\", y_train.shape)\n",
        "\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "print(\"Number of train data - \" + str(len(x_train)))\n",
        "print(\"Number of test data - \" + str(len(x_test)))\n",
        "\n",
        "# Further break training data into train / validation sets (# put 5000 into validation set and keep remaining 55,000 for train)\n",
        "(x_train, x_valid) = x_train[5000:], x_train[:5000] \n",
        "(y_train, y_valid) = y_train[5000:], y_train[:5000]\n",
        "\n",
        "# Reshape input data from (28, 28) to (28, 28, 1)\n",
        "w, h = 28, 28\n",
        "x_train = x_train.reshape(x_train.shape[0], w, h, 1)\n",
        "x_valid = x_valid.reshape(x_valid.shape[0], w, h, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], w, h, 1)\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_valid = tf.keras.utils.to_categorical(y_valid, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Print training set shape\n",
        "print(\"x_train shape:\", x_train.shape, \"y_train shape:\", y_train.shape)\n",
        "\n",
        "# Print the number of training, validation, and test datasets\n",
        "print(x_train.shape[0], 'train set')\n",
        "print(x_valid.shape[0], 'validation set')\n",
        "print(x_test.shape[0], 'test set')\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "# Must define the input shape in the first layer of the neural network\n",
        "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(28,28,1))) \n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
        "model.add(tf.keras.layers.Dropout(0.3))\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
        "model.add(tf.keras.layers.Dropout(0.3))\n",
        "\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Take a look at the model summary\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "\n",
        "#checkpointer = ModelCheckpoint(filepath='cifarmodel.weights.best.hdf5', verbose = 1, save_best_only=True)\n",
        "model.fit(x_train,\n",
        "         y_train,\n",
        "         batch_size=64,\n",
        "         epochs=10,\n",
        "         validation_data=(x_valid, y_valid))\n",
        "\n",
        "model.save('fashionmodel.h5') \n",
        "# Evaluate the model on test set\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "# Print test accuracy\n",
        "print('\\n', 'Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSKEGwGTXJJB",
        "colab_type": "code",
        "outputId": "e201ed3f-a36b-4d29-8f5a-ebfb32e25c77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "# Load the fashion-mnist pre-shuffled train data and test data\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "print(\"x_train shape:\", x_train.shape, \"y_train shape:\", y_train.shape)\n",
        "\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "print(\"Number of train data - \" + str(len(x_train)))\n",
        "print(\"Number of test data - \" + str(len(x_test)))\n",
        "\n",
        "# Further break training data into train / validation sets (# put 5000 into validation set and keep remaining 55,000 for train)\n",
        "(x_train, x_valid) = x_train[5000:], x_train[:5000] \n",
        "(y_train, y_valid) = y_train[5000:], y_train[:5000]\n",
        "\n",
        "# Reshape input data from (28, 28) to (28, 28, 1)\n",
        "w, h = 28, 28\n",
        "x_train = x_train.reshape(x_train.shape[0], w, h, 1)\n",
        "x_valid = x_valid.reshape(x_valid.shape[0], w, h, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], w, h, 1)\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_valid = tf.keras.utils.to_categorical(y_valid, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Print training set shape\n",
        "print(\"x_train shape:\", x_train.shape, \"y_train shape:\", y_train.shape)\n",
        "\n",
        "# Print the number of training, validation, and test datasets\n",
        "print(x_train.shape[0], 'train set')\n",
        "print(x_valid.shape[0], 'validation set')\n",
        "print(x_test.shape[0], 'test set')\n",
        "\n",
        "\n",
        "\n",
        "model = tf.keras.models.load_model(\"fashionmodel.h5\")\n",
        "preds_train = model.predict(x_train, verbose=0)\n",
        "preds_val = model.predict(x_valid, verbose=0)\n",
        "preds = model.predict(x_test, verbose=0)\n",
        "\n",
        "model_copy = model\n",
        "weights = model.get_weights()\n",
        "weights1 = model.get_weights()\n",
        "print(len(weights))\n",
        "#updated_weights = weights[:]\n",
        "#print(\"RESULTS\", len(updated_weights), id(weights), id(updated_weights))\n",
        "#max_val = max(arr.max() for arr in weights)\n",
        "#min_val = min(arr.min() for arr in weights)\n",
        "#print(\"maxval is\",max_val)\n",
        "#print(\"minval is\",min_val)\n",
        "\n",
        "# need to add conditions that allows arrays of different dimensions\n",
        "# Add code to check if it is an element of a list\n",
        "x = 0.0\n",
        "\n",
        "# Change fopr to while an d check it\n",
        "for r1 in range(0, len(weights)):\n",
        "  print(r1)\n",
        "  max_val = max(arr.max() for arr in weights[r1])\n",
        "  min_val = min(arr.min() for arr in weights[r1])\n",
        "  print(\"maxval is\",max_val)\n",
        "  print(\"minval is\",min_val)\n",
        "  if(len(weights[r1]>1)):\n",
        "    #print(\"In r1\")\n",
        "    for r2 in range(0,len(weights[r1])):\n",
        "      #print(type(weights[r1][r2]))\n",
        "      if((type(weights[r1][r2]) is np.ndarray) and len(weights[r1][r2])>1):\n",
        "        #print(\"in r2\")\n",
        "        for r3 in range(0,len(weights[r1][r2])):\n",
        "          weights[r1][r2][r3] = -weights[r1][r2][r3]+1\n",
        "          #print(\"Updated\")\n",
        "      else:\n",
        "        weights[r1][r2] = -weights[r1][r2]+1\n",
        "        # = x\n",
        "        print(weights1[r1][r2], weights[r1][r2])\n",
        "  else:\n",
        "    weights[r1] = -weights[r1]+1\n",
        "   # print(\"Updated\")\n",
        "     \n",
        "model_copy.set_weights(weights)\n",
        "\n",
        "print(\"W!\", len(weights1),len(weights1[1]),(weights1[1][1]))\n",
        "print(\"w\", len(weights),len(weights[1]),(weights[1][1]))\n",
        "\n",
        "\n",
        "#if(weights==weights1):\n",
        "#  print(\"Did not update\")\n",
        "\n",
        "\n",
        "copy_preds_train = model_copy.predict(x_train,verbose=0)\n",
        "print(\"SAMPLES VALUES\")\n",
        "print(copy_preds_train[1])\n",
        "print(copy_preds_train[6])\n",
        "\n",
        "copy_preds_val = model_copy.predict(x_valid, verbose=0)\n",
        "copy_preds = model_copy.predict(x_test, verbose=0)\n",
        "\n",
        "\n",
        "match_with_copy = 0\n",
        "mismatch_with_copy = 0\n",
        "match_with_actual = 0\n",
        "mismatch_with_actual = 0\n",
        "copy_match_with_actual = 0\n",
        "copy_mismatch_with_actual = 0\n",
        "\n",
        "\n",
        "file = open(\"negativeofweightsplus1file_fashionmnisit_regression_test.txt\",\"w\")\n",
        "for ival in range(len(preds)):\n",
        "  for pval in range(0, len(preds[ival])):\n",
        "    file.write(str(preds[ival][pval]))\n",
        "    file.write(\" \")\n",
        "  for pval in range(0,len(copy_preds[ival])):\n",
        "    file.write(str(copy_preds[ival][pval]))\n",
        "    file.write(\" \")\n",
        "  if(np.where(preds[ival] == np.amax(preds[ival]))!=np.where(y_test[ival] == np.amax(y_test[ival]))):\n",
        "    print(\"Writing zero\")\n",
        "    file.write(\"0\")\n",
        "    mismatch_with_actual = mismatch_with_actual+1\n",
        "  else:\n",
        "    file.write(\"1\")\n",
        "    match_with_actual = match_with_actual+1\n",
        "    \n",
        "  file.write(\"\\n\")\n",
        "  \n",
        "file.close()\n",
        "\n",
        "\n",
        "file = open(\"negativeofweightsplus1file_fashionmnisit_regression_input.txt\",\"w\")\n",
        "\n",
        "for ival in range(len(preds_val)):\n",
        "  for pval in range(0, len(preds_val[ival])):\n",
        "    file.write(str(preds_val[ival][pval]))\n",
        "    file.write(\" \")\n",
        "  for pval in range(0,len(copy_preds_val[ival])):\n",
        "    file.write(str(copy_preds_val[ival][pval]))\n",
        "    file.write(\" \")\n",
        "  if(np.where(preds_val[ival] == np.amax(preds_val[ival]))!=np.where(y_valid[ival] == np.amax(y_valid[ival]))):\n",
        "    print(\"Writing zero\")\n",
        "    file.write(\"0\")\n",
        "    mismatch_with_actual = mismatch_with_actual+1\n",
        "  else:\n",
        "    file.write(\"1\")\n",
        "    match_with_actual = match_with_actual+1\n",
        "    \n",
        "  file.write(\"\\n\")\n",
        "  \n",
        "  if(np.where(copy_preds_val[ival] == np.amax(copy_preds_val[ival]))!=np.where(y_valid[ival] == np.amax(y_valid[ival]))):\n",
        "    copy_mismatch_with_actual = copy_mismatch_with_actual + 1\n",
        "  else:\n",
        "    copy_match_with_actual = copy_match_with_actual + 1\n",
        "    \n",
        "  if(np.where(preds_val[ival] == np.amax(preds_val[ival]))!=np.where(copy_preds_val[ival] == np.amax(copy_preds_val[ival]))):\n",
        "    mismatch_with_copy = mismatch_with_copy+1\n",
        "  else:\n",
        "    match_with_copy = match_with_copy+1    \n",
        "    #print(np.where(preds_val[ival] == np.amax(preds_val[ival])), np.where(copy_preds_val[ival] == np.amax(copy_preds_val[ival])), np.where(y_valid[ival] == np.amax(y_valid[ival,\n",
        "    \n",
        "print(\"Results\")\n",
        "print(\"True Mode vs Actual\")\n",
        "print(\"Match = \", match_with_actual)\n",
        "print(\"Mismatch = \", mismatch_with_actual)\n",
        "\n",
        "print(\"Copy Mode vs Actual\")\n",
        "print(\"Match = \", copy_match_with_actual)\n",
        "print(\"Mismatch = \", copy_mismatch_with_actual)\n",
        "\n",
        "print(\"Copy Mode vs True Mode\")\n",
        "print(\"Match = \", match_with_copy)\n",
        "print(\"Mismatch = \", mismatch_with_copy)\n",
        "\n",
        "\n",
        "match_with_copy = 0\n",
        "mismatch_with_copy = 0\n",
        "match_with_actual = 0\n",
        "mismatch_with_actual = 0\n",
        "copy_match_with_actual = 0\n",
        "copy_mismatch_with_actual = 0\n",
        "\n",
        "\n",
        "\n",
        "for ival in range(0,len(preds_train)):\n",
        "  for pval in range(0, len(preds_train[ival])):\n",
        "    file.write(str(preds_train[ival][pval]))\n",
        "    file.write(\" \")\n",
        "  for pval in range(0,len(copy_preds_train[ival])):\n",
        "    file.write(str(copy_preds_train[ival][pval]))\n",
        "    file.write(\" \")\n",
        "  if(np.where(preds_train[ival] == np.amax(preds_train[ival]))!=np.where(y_train[ival] == np.amax(y_train[ival]))):\n",
        "    file.write(\"0\")\n",
        "    mismatch_with_actual = mismatch_with_actual+1\n",
        "  else:\n",
        "    file.write(\"1\")\n",
        "    match_with_actual = match_with_actual+1\n",
        "   \n",
        "  file.write(\"\\n\")\n",
        "  if(np.where(copy_preds_train[ival] == np.amax(copy_preds_train[ival]))!=np.where(y_train[ival] == np.amax(y_train[ival]))):\n",
        "    copy_mismatch_with_actual = copy_mismatch_with_actual + 1\n",
        "  else:\n",
        "    copy_match_with_actual = copy_match_with_actual + 1\n",
        "  if(np.where(preds_train[ival] == np.amax(preds_train[ival]))!=np.where(copy_preds_train[ival] == np.amax(copy_preds_train[ival]))):\n",
        "    mismatch_with_copy = mismatch_with_copy+1\n",
        "  else:\n",
        "    match_with_copy = match_with_copy+1    \n",
        "    #print(np.where(preds_val[ival] == np.amax(preds_val[ival])), np.where(copy_preds_val[ival] == np.amax(copy_preds_val[ival])), np.where(y_valid[ival] == np.amax(y_valid[ival,\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Training Results\")\n",
        "print(\"True Mode vs Actual\")\n",
        "print(\"Match = \", match_with_actual)\n",
        "print(\"Mismatch = \", mismatch_with_actual)\n",
        "\n",
        "print(\"Copy Mode vs Actual\")\n",
        "print(\"Match = \", copy_match_with_actual)\n",
        "print(\"Mismatch = \", copy_mismatch_with_actual)\n",
        "\n",
        "print(\"Copy Mode vs True Mode\")\n",
        "print(\"Match = \", match_with_copy)\n",
        "print(\"Mismatch = \", mismatch_with_copy)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0807 23:39:25.266720 140511543216000 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/init_ops.py:97: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0807 23:39:25.269125 140511543216000 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/init_ops.py:97: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28) y_train shape: (60000,)\n",
            "Number of train data - 60000\n",
            "Number of test data - 10000\n",
            "x_train shape: (55000, 28, 28, 1) y_train shape: (55000, 10)\n",
            "55000 train set\n",
            "5000 validation set\n",
            "10000 test set\n",
            "8\n",
            "0\n",
            "maxval is 0.326725\n",
            "minval is -1.4955378\n",
            "1\n",
            "maxval is 0.09052338\n",
            "minval is -0.12863117\n",
            "-0.009192906 1.009193\n",
            "-0.003614593 1.0036145\n",
            "-0.004605825 1.0046058\n",
            "0.00036139798 0.9996386\n",
            "-0.0076865996 1.0076866\n",
            "-0.0048927385 1.0048927\n",
            "-0.0141465515 1.0141466\n",
            "-0.004879119 1.0048791\n",
            "-0.114095844 1.1140958\n",
            "-0.004091095 1.0040911\n",
            "0.0 1.0\n",
            "-0.006307797 1.0063078\n",
            "-0.0135139115 1.0135139\n",
            "-0.0041963323 1.0041963\n",
            "-0.0427475 1.0427475\n",
            "-0.008941204 1.0089412\n",
            "-0.0019980485 1.0019981\n",
            "-0.00862023 1.0086203\n",
            "-0.048033744 1.0480337\n",
            "-0.056371555 1.0563716\n",
            "-0.011426947 1.0114269\n",
            "-0.009247185 1.0092472\n",
            "0.055233862 0.94476616\n",
            "6.7456305e-05 0.9999325\n",
            "0.004964113 0.9950359\n",
            "-0.0074657267 1.0074657\n",
            "0.0006031422 0.99939686\n",
            "0.012183299 0.9878167\n",
            "-0.0016485755 1.0016485\n",
            "0.03726586 0.96273416\n",
            "-0.0040579033 1.0040579\n",
            "0.0 1.0\n",
            "-0.08878452 1.0887846\n",
            "-0.0037543767 1.0037544\n",
            "-0.0024558506 1.0024558\n",
            "0.0021199563 0.99788004\n",
            "0.0061282986 0.9938717\n",
            "0.0 1.0\n",
            "-0.07340894 1.073409\n",
            "0.0031743154 0.9968257\n",
            "-0.014459766 1.0144597\n",
            "-0.0045792684 1.0045793\n",
            "-0.004952743 1.0049528\n",
            "0.031433526 0.9685665\n",
            "0.010967673 0.9890323\n",
            "-3.4662415e-05 1.0000347\n",
            "-0.001469247 1.0014693\n",
            "0.00043462057 0.99956536\n",
            "0.070195325 0.9298047\n",
            "-0.0060038907 1.0060039\n",
            "-0.0060627917 1.0060627\n",
            "-0.004324128 1.0043241\n",
            "-0.0029580367 1.002958\n",
            "0.039221056 0.96077895\n",
            "-0.008197846 1.0081979\n",
            "-0.014572112 1.0145721\n",
            "-0.0075126775 1.0075127\n",
            "0.06709218 0.9329078\n",
            "-0.064647175 1.0646472\n",
            "-0.0016775756 1.0016776\n",
            "-0.12863117 1.1286311\n",
            "-0.00786103 1.007861\n",
            "0.09052338 0.90947664\n",
            "0.036873706 0.9631263\n",
            "2\n",
            "maxval is 0.6044249\n",
            "minval is -0.8471404\n",
            "3\n",
            "maxval is 0.1836481\n",
            "minval is -0.23016702\n",
            "-0.023147665 1.0231477\n",
            "-0.13570394 1.1357039\n",
            "-0.14942993 1.1494299\n",
            "0.15989423 0.8401058\n",
            "-0.089724205 1.0897242\n",
            "-0.05483223 1.0548322\n",
            "0.08475806 0.91524196\n",
            "-0.023495208 1.0234952\n",
            "0.056578185 0.94342184\n",
            "-0.19136088 1.1913608\n",
            "-0.03677287 1.0367728\n",
            "-0.012924735 1.0129248\n",
            "-0.02663277 1.0266328\n",
            "-0.09249485 1.0924948\n",
            "-0.09103952 1.0910395\n",
            "0.1836481 0.8163519\n",
            "-0.011927597 1.0119276\n",
            "-0.23016702 1.230167\n",
            "-0.131641 1.131641\n",
            "-0.13309969 1.1330997\n",
            "-0.107189566 1.1071895\n",
            "0.009745762 0.9902542\n",
            "-0.11169497 1.1116949\n",
            "-0.02186002 1.02186\n",
            "0.014763002 0.985237\n",
            "-0.02316349 1.0231634\n",
            "0.03171311 0.9682869\n",
            "0.04762719 0.9523728\n",
            "-0.04977522 1.0497752\n",
            "0.047537588 0.95246243\n",
            "0.1404632 0.85953677\n",
            "0.068216205 0.9317838\n",
            "4\n",
            "maxval is 0.6063449\n",
            "minval is -0.70366794\n",
            "5\n",
            "maxval is 0.19221646\n",
            "minval is -0.100086026\n",
            "0.025498675 0.9745013\n",
            "0.027891912 0.97210807\n",
            "-0.014344368 1.0143443\n",
            "-0.012946049 1.012946\n",
            "0.07380798 0.92619205\n",
            "0.08738149 0.9126185\n",
            "0.10670065 0.89329934\n",
            "-0.051265474 1.0512655\n",
            "-0.026401658 1.0264016\n",
            "0.014247564 0.98575246\n",
            "-0.049585886 1.0495859\n",
            "0.0039430237 0.996057\n",
            "-0.00785503 1.007855\n",
            "0.07113258 0.9288674\n",
            "0.021152444 0.97884756\n",
            "0.08050554 0.91949445\n",
            "0.0020991059 0.9979009\n",
            "0.025940493 0.9740595\n",
            "-0.0015100488 1.00151\n",
            "-0.02023566 1.0202357\n",
            "0.06352514 0.93647486\n",
            "0.047100563 0.95289946\n",
            "-0.01574993 1.0157499\n",
            "0.03978135 0.96021867\n",
            "0.007665298 0.9923347\n",
            "-0.03462929 1.0346293\n",
            "0.09393052 0.90606946\n",
            "-0.014577149 1.0145772\n",
            "0.06479713 0.93520284\n",
            "0.009105902 0.9908941\n",
            "-0.015271227 1.0152712\n",
            "0.022379233 0.9776208\n",
            "0.053809978 0.94619\n",
            "-0.0065455153 1.0065455\n",
            "0.015510963 0.984489\n",
            "0.06440766 0.93559235\n",
            "0.10947842 0.8905216\n",
            "-0.028619424 1.0286194\n",
            "0.16314293 0.8368571\n",
            "0.07384378 0.9261562\n",
            "-0.022240132 1.0222402\n",
            "0.048904445 0.9510956\n",
            "0.0073626065 0.9926374\n",
            "0.0029693579 0.9970306\n",
            "-0.051096562 1.0510966\n",
            "0.13127081 0.8687292\n",
            "0.00791667 0.9920833\n",
            "-0.048184615 1.0481846\n",
            "0.19221646 0.80778354\n",
            "-0.0038054907 1.0038055\n",
            "-0.02532412 1.0253241\n",
            "-0.034784637 1.0347847\n",
            "0.06741725 0.93258274\n",
            "0.047351267 0.95264876\n",
            "0.014887637 0.98511237\n",
            "0.001316057 0.9986839\n",
            "0.06457766 0.93542236\n",
            "-0.075497955 1.075498\n",
            "0.039993804 0.9600062\n",
            "-0.010951453 1.0109514\n",
            "-0.007068792 1.0070688\n",
            "0.038826168 0.96117383\n",
            "0.08004824 0.91995174\n",
            "0.09098246 0.90901756\n",
            "0.08323046 0.91676956\n",
            "0.0017192375 0.99828076\n",
            "0.07432653 0.9256735\n",
            "0.033917565 0.96608245\n",
            "0.1438833 0.8561167\n",
            "0.06412535 0.93587464\n",
            "0.029729605 0.9702704\n",
            "0.02950021 0.9704998\n",
            "-0.016469447 1.0164695\n",
            "0.000705129 0.9992949\n",
            "0.015543741 0.98445624\n",
            "0.10910216 0.89089787\n",
            "0.017726045 0.98227394\n",
            "-0.054119222 1.0541192\n",
            "0.03303543 0.96696454\n",
            "0.053914797 0.9460852\n",
            "-0.06712928 1.0671293\n",
            "0.098713905 0.9012861\n",
            "0.011256656 0.98874336\n",
            "-0.024893142 1.0248932\n",
            "0.065907374 0.93409264\n",
            "0.09360697 0.90639305\n",
            "-0.031390496 1.0313905\n",
            "-0.0060479124 1.006048\n",
            "-0.049333565 1.0493336\n",
            "-0.009311738 1.0093118\n",
            "0.09381905 0.906181\n",
            "-0.100086026 1.100086\n",
            "0.04244385 0.9575561\n",
            "-0.02668746 1.0266875\n",
            "0.02098198 0.97901803\n",
            "0.11519493 0.8848051\n",
            "0.118019454 0.88198054\n",
            "-0.008229426 1.0082294\n",
            "0.026439229 0.97356075\n",
            "0.018292574 0.98170745\n",
            "0.10200943 0.8979906\n",
            "0.013251878 0.9867481\n",
            "-0.052093882 1.0520939\n",
            "-0.038140006 1.03814\n",
            "0.065098844 0.9349012\n",
            "-0.022145765 1.0221457\n",
            "0.04276484 0.95723516\n",
            "0.030637195 0.9693628\n",
            "-0.0048169545 1.004817\n",
            "0.07909311 0.9209069\n",
            "0.045258064 0.95474195\n",
            "0.063125245 0.93687475\n",
            "0.008258501 0.9917415\n",
            "-0.014577904 1.0145779\n",
            "0.009264932 0.99073505\n",
            "-0.019502623 1.0195026\n",
            "0.0631872 0.9368128\n",
            "0.09971549 0.9002845\n",
            "-0.030736554 1.0307366\n",
            "0.06553436 0.93446565\n",
            "0.100308485 0.8996915\n",
            "0.03555662 0.9644434\n",
            "-0.034003217 1.0340033\n",
            "-0.010910317 1.0109103\n",
            "0.057932995 0.942067\n",
            "0.09510176 0.9048982\n",
            "0.04675411 0.9532459\n",
            "0.10975432 0.8902457\n",
            "0.026671568 0.9733284\n",
            "0.11010975 0.88989025\n",
            "-0.027712328 1.0277123\n",
            "0.06866796 0.93133205\n",
            "0.033378057 0.96662194\n",
            "0.096651904 0.9033481\n",
            "0.10635226 0.89364773\n",
            "0.016138474 0.9838615\n",
            "-0.013406567 1.0134065\n",
            "-0.06040127 1.0604013\n",
            "0.048132695 0.9518673\n",
            "0.069306746 0.93069327\n",
            "0.09612503 0.903875\n",
            "0.078255475 0.9217445\n",
            "-0.055881836 1.0558819\n",
            "0.0024921354 0.99750787\n",
            "0.06320799 0.936792\n",
            "0.009258271 0.9907417\n",
            "0.04035306 0.95964694\n",
            "0.085136086 0.91486394\n",
            "0.05921415 0.9407858\n",
            "0.12955205 0.87044793\n",
            "-0.026783697 1.0267837\n",
            "-0.031624526 1.0316246\n",
            "-0.027293222 1.0272932\n",
            "-0.0061161914 1.0061162\n",
            "0.041369103 0.9586309\n",
            "0.0070510875 0.9929489\n",
            "0.075710736 0.9242893\n",
            "-0.0046778563 1.0046779\n",
            "0.08401516 0.91598487\n",
            "0.0906788 0.9093212\n",
            "-0.02736607 1.027366\n",
            "0.04579124 0.95420873\n",
            "0.05467083 0.9453292\n",
            "0.00433837 0.9956616\n",
            "0.13503923 0.8649608\n",
            "-0.010915947 1.010916\n",
            "-0.028553948 1.028554\n",
            "-0.034640677 1.0346407\n",
            "0.008278011 0.991722\n",
            "0.07058819 0.9294118\n",
            "0.045149166 0.95485085\n",
            "-0.0029756487 1.0029757\n",
            "0.001258996 0.99874103\n",
            "0.060677372 0.93932265\n",
            "-0.047269046 1.0472691\n",
            "0.00237432 0.9976257\n",
            "-0.009658572 1.0096586\n",
            "-0.010623595 1.0106236\n",
            "-0.008796179 1.0087962\n",
            "-0.006588477 1.0065885\n",
            "0.08817483 0.9118252\n",
            "0.09557772 0.9044223\n",
            "0.051723633 0.94827634\n",
            "0.050494745 0.94950527\n",
            "0.053602096 0.9463979\n",
            "0.059467863 0.94053215\n",
            "0.035274297 0.96472573\n",
            "0.090589434 0.9094106\n",
            "0.05689808 0.94310194\n",
            "-0.033679236 1.0336792\n",
            "0.024670029 0.97533\n",
            "0.03486311 0.9651369\n",
            "0.009342134 0.99065787\n",
            "-0.008937947 1.008938\n",
            "0.044298105 0.9557019\n",
            "0.0627251 0.93727493\n",
            "0.10484987 0.8951501\n",
            "0.09731689 0.90268314\n",
            "0.0845434 0.9154566\n",
            "0.07157391 0.9284261\n",
            "0.05406095 0.94593906\n",
            "-0.022897474 1.0228975\n",
            "0.035823833 0.9641762\n",
            "-0.054937977 1.054938\n",
            "0.09013361 0.9098664\n",
            "0.07390752 0.9260925\n",
            "0.033746835 0.96625316\n",
            "0.13011618 0.86988384\n",
            "0.1327778 0.8672222\n",
            "-0.0083607845 1.0083607\n",
            "-0.009273503 1.0092735\n",
            "-0.0135652665 1.0135653\n",
            "-0.022931825 1.0229318\n",
            "0.057116956 0.942883\n",
            "0.09051963 0.9094804\n",
            "-0.047361337 1.0473614\n",
            "-0.01797549 1.0179754\n",
            "0.096188545 0.90381145\n",
            "0.039011687 0.9609883\n",
            "0.12766886 0.87233114\n",
            "0.021427454 0.97857255\n",
            "0.062303506 0.9376965\n",
            "0.030576218 0.9694238\n",
            "0.10707705 0.89292294\n",
            "0.14097832 0.85902166\n",
            "-0.03696539 1.0369654\n",
            "-0.016779363 1.0167794\n",
            "0.101229556 0.89877045\n",
            "0.0074436036 0.9925564\n",
            "-0.026526079 1.0265261\n",
            "0.10642241 0.8935776\n",
            "0.026140677 0.9738593\n",
            "0.11930968 0.88069034\n",
            "0.053919397 0.9460806\n",
            "-0.05446653 1.0544665\n",
            "0.114553474 0.88544655\n",
            "-0.009943479 1.0099435\n",
            "0.028709365 0.97129065\n",
            "0.016760495 0.98323953\n",
            "0.0075001875 0.9924998\n",
            "0.069496036 0.93050396\n",
            "-0.056847323 1.0568473\n",
            "0.026632696 0.97336733\n",
            "0.04844468 0.9515553\n",
            "0.10339023 0.8966098\n",
            "0.054142192 0.9458578\n",
            "-0.028302087 1.0283021\n",
            "0.015567024 0.984433\n",
            "-0.009714618 1.0097146\n",
            "0.0019031191 0.9980969\n",
            "-0.018005919 1.018006\n",
            "0.071581386 0.92841864\n",
            "0.05513177 0.9448682\n",
            "-0.019441983 1.019442\n",
            "0.14229324 0.8577068\n",
            "0.057678893 0.9423211\n",
            "6\n",
            "maxval is 0.43076244\n",
            "minval is -0.7121\n",
            "7\n",
            "maxval is 0.07499901\n",
            "minval is -0.268992\n",
            "0.071524195 0.9284758\n",
            "-0.268992 1.268992\n",
            "0.021189718 0.9788103\n",
            "0.07499901 0.92500097\n",
            "-0.00035336017 1.0003533\n",
            "-0.124549754 1.1245497\n",
            "0.07217467 0.92782533\n",
            "0.056694254 0.94330573\n",
            "-0.09353355 1.0935335\n",
            "-0.10948015 1.1094801\n",
            "W! 8 64 -0.003614593\n",
            "w 8 64 1.0036145\n",
            "SAMPLES VALUES\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Writing zero\n",
            "Results\n",
            "True Mode vs Actual\n",
            "Match =  13670\n",
            "Mismatch =  1330\n",
            "Copy Mode vs Actual\n",
            "Match =  506\n",
            "Mismatch =  4494\n",
            "Copy Mode vs True Mode\n",
            "Match =  509\n",
            "Mismatch =  4491\n",
            "\n",
            "\n",
            "Training Results\n",
            "True Mode vs Actual\n",
            "Match =  51173\n",
            "Mismatch =  3827\n",
            "Copy Mode vs Actual\n",
            "Match =  5494\n",
            "Mismatch =  49506\n",
            "Copy Mode vs True Mode\n",
            "Match =  5539\n",
            "Mismatch =  49461\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0J9JGMjxJ4um",
        "colab_type": "code",
        "outputId": "654af397-dfa0-41ed-e021-8e18b9c43df3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "from keras import backend as K\n",
        "from keras.layers import Layer\n",
        "from keras import activations\n",
        "from keras import utils\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Model\n",
        "from keras.layers import *\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "# the squashing function.\n",
        "# we use 0.5 in stead of 1 in hinton's paper.\n",
        "# if 1, the norm of vector will be zoomed out.\n",
        "# if 0.5, the norm will be zoomed in while original norm is less than 0.5\n",
        "# and be zoomed out while original norm is greater than 0.5.\n",
        "def squash(x, axis=-1):\n",
        "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
        "    scale = K.sqrt(s_squared_norm) / (0.5 + s_squared_norm)\n",
        "    return scale * x\n",
        "\n",
        "\n",
        "# define our own softmax function instead of K.softmax\n",
        "# because K.softmax can not specify axis.\n",
        "def softmax(x, axis=-1):\n",
        "    ex = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
        "    return ex / K.sum(ex, axis=axis, keepdims=True)\n",
        "\n",
        "\n",
        "# define the margin loss like hinge loss\n",
        "def margin_loss(y_true, y_pred):\n",
        "    lamb, margin = 0.5, 0.1\n",
        "    return K.sum(y_true * K.square(K.relu(1 - margin - y_pred)) + lamb * (\n",
        "        1 - y_true) * K.square(K.relu(y_pred - margin)), axis=-1)\n",
        "\n",
        "\n",
        "class Capsule(Layer):\n",
        "    \"\"\"A Capsule Implement with Pure Keras\n",
        "    There are two vesions of Capsule.\n",
        "    One is like dense layer (for the fixed-shape input),\n",
        "    and the other is like timedistributed dense (for various length input).\n",
        "\n",
        "    The input shape of Capsule must be (batch_size,\n",
        "                                        input_num_capsule,\n",
        "                                        input_dim_capsule\n",
        "                                       )\n",
        "    and the output shape is (batch_size,\n",
        "                             num_capsule,\n",
        "                             dim_capsule\n",
        "                            )\n",
        "\n",
        "    Capsule Implement is from https://github.com/bojone/Capsule/\n",
        "    Capsule Paper: https://arxiv.org/abs/1710.09829\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_capsule,\n",
        "                 dim_capsule,\n",
        "                 routings=3,\n",
        "                 share_weights=True,\n",
        "                 activation='squash',\n",
        "                 **kwargs):\n",
        "        super(Capsule, self).__init__(**kwargs)\n",
        "        self.num_capsule = num_capsule\n",
        "        self.dim_capsule = dim_capsule\n",
        "        self.routings = routings\n",
        "        self.share_weights = share_weights\n",
        "        if activation == 'squash':\n",
        "            self.activation = squash\n",
        "        else:\n",
        "            self.activation = activations.get(activation)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_dim_capsule = input_shape[-1]\n",
        "        if self.share_weights:\n",
        "            self.kernel = self.add_weight(\n",
        "                name='capsule_kernel',\n",
        "                shape=(1, input_dim_capsule,\n",
        "                       self.num_capsule * self.dim_capsule),\n",
        "                initializer='glorot_uniform',\n",
        "                trainable=True)\n",
        "        else:\n",
        "            input_num_capsule = input_shape[-2]\n",
        "            self.kernel = self.add_weight(\n",
        "                name='capsule_kernel',\n",
        "                shape=(input_num_capsule, input_dim_capsule,\n",
        "                       self.num_capsule * self.dim_capsule),\n",
        "                initializer='glorot_uniform',\n",
        "                trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Following the routing algorithm from Hinton's paper,\n",
        "        but replace b = b + <u,v> with b = <u,v>.\n",
        "\n",
        "        This change can improve the feature representation of Capsule.\n",
        "\n",
        "        However, you can replace\n",
        "            b = K.batch_dot(outputs, hat_inputs, [2, 3])\n",
        "        with\n",
        "            b += K.batch_dot(outputs, hat_inputs, [2, 3])\n",
        "        to realize a standard routing.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.share_weights:\n",
        "            hat_inputs = K.conv1d(inputs, self.kernel)\n",
        "        else:\n",
        "            hat_inputs = K.local_conv1d(inputs, self.kernel, [1], [1])\n",
        "\n",
        "        batch_size = K.shape(inputs)[0]\n",
        "        input_num_capsule = K.shape(inputs)[1]\n",
        "        hat_inputs = K.reshape(hat_inputs,\n",
        "                               (batch_size, input_num_capsule,\n",
        "                                self.num_capsule, self.dim_capsule))\n",
        "        hat_inputs = K.permute_dimensions(hat_inputs, (0, 2, 1, 3))\n",
        "\n",
        "        b = K.zeros_like(hat_inputs[:, :, :, 0])\n",
        "        for i in range(self.routings):\n",
        "            c = softmax(b, 1)\n",
        "            o = self.activation(K.batch_dot(c, hat_inputs, [2, 2]))\n",
        "            if i < self.routings - 1:\n",
        "                b = K.batch_dot(o, hat_inputs, [2, 3])\n",
        "                if K.backend() == 'theano':\n",
        "                    o = K.sum(o, axis=1)\n",
        "\n",
        "        return o\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (None, self.num_capsule, self.dim_capsule)\n",
        "\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 100\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "y_train = utils.to_categorical(y_train, num_classes)\n",
        "y_test = utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# A common Conv2D model\n",
        "input_image = Input(shape=(None, None, 3))\n",
        "x = Conv2D(64, (3, 3), activation='relu')(input_image)\n",
        "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
        "x = AveragePooling2D((2, 2))(x)\n",
        "x = Conv2D(128, (3, 3), activation='relu')(x)\n",
        "x = Conv2D(128, (3, 3), activation='relu')(x)\n",
        "\n",
        "\n",
        "\"\"\"now we reshape it as (batch_size, input_num_capsule, input_dim_capsule)\n",
        "then connect a Capsule layer.\n",
        "\n",
        "the output of final model is the lengths of 10 Capsule, whose dim=16.\n",
        "\n",
        "the length of Capsule is the proba,\n",
        "so the problem becomes a 10 two-classification problem.\n",
        "\"\"\"\n",
        "\n",
        "x = Reshape((-1, 128))(x)\n",
        "capsule = Capsule(10, 16, 3, True)(x)\n",
        "output = Lambda(lambda z: K.sqrt(K.sum(K.square(z), 2)))(capsule)\n",
        "model = Model(inputs=input_image, outputs=output)\n",
        "\n",
        "# we use a margin loss\n",
        "model.compile(loss=margin_loss, optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# we can compare the performance with or without data augmentation\n",
        "data_augmentation = True\n",
        "\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_data=(x_test, y_test),\n",
        "        shuffle=True)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by dataset std\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
        "        rotation_range=0,  # randomly rotate images in 0 to 180 degrees\n",
        "        width_shift_range=0.1,  # randomly shift images horizontally\n",
        "        height_shift_range=0.1,  # randomly shift images vertically\n",
        "        shear_range=0.,  # set range for random shear\n",
        "        zoom_range=0.,  # set range for random zoom\n",
        "        channel_shift_range=0.,  # set range for random channel shifts\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        cval=0.,  # value used for fill_mode = \"constant\"\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False,  # randomly flip images\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.15)\n",
        "\n",
        "    # Compute quantities required for feature-wise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # Fit the model on the batches generated by datagen.flow().\n",
        "    model.fit_generator(\n",
        "        datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "        epochs=epochs,\n",
        "        steps_per_epoch=len(x_train)/batch_size,\n",
        "        workers=4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 4s 0us/step\n",
            "170508288/170498071 [==============================] - 4s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0809 16:30:47.415662 140114948638592 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0809 16:30:47.448395 140114948638592 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0809 16:30:47.458969 140114948638592 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0809 16:30:47.513297 140114948638592 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "W0809 16:30:47.770030 140114948638592 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, None, None, 3)     0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, None, None, 64)    1792      \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, None, None, 64)    36928     \n",
            "_________________________________________________________________\n",
            "average_pooling2d_1 (Average (None, None, None, 64)    0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, None, None, 128)   73856     \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, None, None, 128)   147584    \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, None, 128)         0         \n",
            "_________________________________________________________________\n",
            "capsule_1 (Capsule)          (None, 10, 16)            20480     \n",
            "_________________________________________________________________\n",
            "lambda_1 (Lambda)            (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 280,640\n",
            "Trainable params: 280,640\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Using real-time data augmentation.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0809 16:30:48.585537 140114948638592 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_grad.py:1250: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0809 16:30:49.073255 140114948638592 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "W0809 16:30:49.201735 140114948638592 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "184/390 [=============>................] - ETA: 4:14 - loss: 0.4646 - acc: 0.2716"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}